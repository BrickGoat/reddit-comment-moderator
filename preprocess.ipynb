{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "subreddits = [\"kanye\", \"askScience\", \"PoliticalDiscussion\", \"socialism\"]\n",
    "\n",
    "#comments are pulled from the top 40 posts from the past month in each subreddit.\n",
    "dataframes = []\n",
    "for sub in subreddits:\n",
    "    df = pd.read_csv(f\"data/comments_{sub}.csv\")\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['postTag', 'user', 'comment_score', 'comment_body', 'mod_deleted',\n",
      "       'user_deleted', 'verified', 'is_gold', 'has_verified_email',\n",
      "       'link_karma', 'total_karma', 'created_utc', 'comment_karma'],\n",
      "      dtype='object')\n",
      "kanye\n",
      "(42654, 13)\n",
      "askScience\n",
      "(11134, 13)\n",
      "PoliticalDiscussion\n",
      "(31235, 13)\n",
      "socialism\n",
      "(3038, 13)\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].columns)\n",
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop comments that were removed too quickly and so were not archived. Also drop comments automatically generated by mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "(40436, 13)\n",
      "askScience\n",
      "(6679, 13)\n",
      "PoliticalDiscussion\n",
      "(28026, 13)\n",
      "socialism\n",
      "(2620, 13)\n"
     ]
    }
   ],
   "source": [
    "urlRegex = r\"(https? *:*\\/*\\/*)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\"\n",
    "mods = [\"https://www.reddit.com/user/AutoModerator\", \"https://www.reddit.com/user/socialism-ModTeam\"]\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].dropna(subset=[\"user\"])\n",
    "    dataframes[i] = dataframes[i][~dataframes[i][\"user\"].isin(mods)]\n",
    "    for col in ['verified', 'is_gold', 'has_verified_email']:\n",
    "        dataframes[i][col] = dataframes[i][col].apply(lambda x: 1 if x else 0)\n",
    "    dataframes[i]['comment_body'] = dataframes[i]['comment_body'].apply(lambda x: re.sub(urlRegex, ' ', x))\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target is very uneven so we need to make sure we don't overfit on the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "mod_deleted\n",
      "0    39584\n",
      "1      852\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    38187\n",
      "1     2249\n",
      "dtype: int64\n",
      "\n",
      "askScience\n",
      "mod_deleted\n",
      "0    5146\n",
      "1    1533\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    6570\n",
      "1     109\n",
      "dtype: int64\n",
      "\n",
      "PoliticalDiscussion\n",
      "mod_deleted\n",
      "0    27189\n",
      "1      837\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    27361\n",
      "1      665\n",
      "dtype: int64\n",
      "\n",
      "socialism\n",
      "mod_deleted\n",
      "0    2392\n",
      "1     228\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    2559\n",
      "1      61\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].groupby([\"mod_deleted\"]).size())\n",
    "    print(dataframes[i].groupby([\"user_deleted\"]).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "['actual' 'also' 'ani' 'anti' 'becaus' 'black' 'call' 'even' 'fuck' 'get'\n",
      " 'go' 'good' 'hate' 'jew' 'jewish' 'kany' 'know' 'like' 'lol' 'make' 'man'\n",
      " 'mean' 'music' 'need' 'one' 'onli' 'peopl' 'person' 'point' 'realli'\n",
      " 'right' 'said' 'say' 'see' 'shit' 'someon' 'still' 'take' 'talk' 'thing'\n",
      " 'think' 'time' 'tri' 'use' 'want' 'way' 'whi' 'white' 'would' 'ye']\n",
      "\n",
      "['act like' 'anti semit' 'black peopl' 'black people' 'death con'\n",
      " 'eric andr' 'feel like' 'georg floyd' 'get help' 'jewish peopl'\n",
      " 'jewish people' 'kany said' 'kany west' 'like kany' 'look like'\n",
      " 'mental health' 'mental ill' 'need help' 'peopl like' 'piec shit'\n",
      " 'seem like' 'social media' 'sound like' 'white peopl' 'year old']\n",
      "\n",
      "askScience\n",
      "['actual' 'also' 'ani' 'becaus' 'bodi' 'caus' 'cell' 'could' 'differ'\n",
      " 'doe' 'earth' 'effect' 'enough' 'even' 'get' 'go' 'human' 'know' 'like'\n",
      " 'long' 'look' 'lot' 'make' 'mani' 'mean' 'much' 'need' 'one' 'onli'\n",
      " 'peopl' 'pressur' 'realli' 'say' 'see' 'someth' 'still' 'system' 'take'\n",
      " 'thing' 'think' 'time' 'use' 'veri' 'water' 'way' 'well' 'whi' 'work'\n",
      " 'would' 'year']\n",
      "\n",
      "['black hole' 'blood cell' 'dark matter' 'et al' 'event horizon'\n",
      " 'everi day' 'feel like' 'immun respons' 'immun system' 'long term'\n",
      " 'long time' 'look like' 'lose weight' 'million year' 'neutron star'\n",
      " 'pretti much' 'seem like' 'solar system' 'someth like' 'sound like'\n",
      " 'thing like' 'water pressur' 'water tower' 'would take' 'year ago']\n",
      "\n",
      "PoliticalDiscussion\n",
      "['actual' 'also' 'ani' 'becaus' 'biden' 'could' 'crime' 'democrat' 'elect'\n",
      " 'even' 'get' 'go' 'good' 'gop' 'know' 'like' 'make' 'mani' 'mean' 'much'\n",
      " 'need' 'one' 'onli' 'parti' 'peopl' 'point' 'polit' 'realli' 'republican'\n",
      " 'right' 'say' 'see' 'state' 'still' 'take' 'thing' 'think' 'time' 'tri'\n",
      " 'trump' 'us' 'use' 'vote' 'voter' 'want' 'way' 'whi' 'work' 'would'\n",
      " 'year']\n",
      "\n",
      "['affirm action' 'even though' 'feel like' 'look like' 'lot peopl'\n",
      " 'mani peopl' 'onli one' 'peopl like' 'peopl vote' 'peopl want'\n",
      " 'popular vote' 'pretti much' 'republican parti' 'right wing' 'seem like'\n",
      " 'social media' 'sound like' 'student loan' 'suprem court' 'tax cut'\n",
      " 'thing like' 'vote democrat' 'vote republican' 'year ago' 'young peopl']\n",
      "\n",
      "socialism\n",
      "['also' 'american' 'ani' 'becaus' 'capit' 'class' 'countri' 'even' 'fuck'\n",
      " 'get' 'go' 'good' 'know' 'like' 'look' 'make' 'mean' 'much' 'need'\n",
      " 'never' 'one' 'onli' 'peopl' 'point' 'power' 'protest' 'realli' 'right'\n",
      " 'say' 'see' 'social' 'socialist' 'state' 'still' 'support' 'take' 'thing'\n",
      " 'think' 'time' 'us' 'use' 'veri' 'want' 'way' 'well' 'whi' 'work' 'world'\n",
      " 'would' 'year']\n",
      "\n",
      "['charli kirk' 'civil war' 'communist parti' 'follow letter' 'free speech'\n",
      " 'left wing' 'letter follow' 'like say' 'look like' 'lot peopl'\n",
      " 'make sure' 'mani peopl' 'onli one' 'peopl like' 'right wing'\n",
      " 'rule class' 'seem like' 'sound like' 'south asian' 'think would'\n",
      " 'unit state' 'unit states' 'work class' 'year ago' 'year old']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "comments_trans = []\n",
    "\n",
    "# Added stemming to vectorization but may not be necessary, max_df has a bigger impact\n",
    "porter = SnowballStemmer(\"english\")\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    comments = dataframes[i][\"comment_body\"].apply(lambda x: \" \".join(tokenizer_porter(x)))\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(subreddits[i])\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n",
    "    comments_trans.append(vect.transform(comments).toarray())\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(2,2) ,max_features=25, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kanyeData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer, TfidfTransformer\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m make_pipeline\n\u001b[1;32m----> 5\u001b[0m kanye_tfidf \u001b[39m=\u001b[39m make_pipeline(CountVectorizer(stop_words\u001b[39m=\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m), TfidfTransformer())\u001b[39m.\u001b[39mfit_transform(kanyeData[\u001b[39m'\u001b[39m\u001b[39mcomment_body\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x)))\n\u001b[0;32m      6\u001b[0m kanye_tfidf\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kanyeData' is not defined"
     ]
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "x = ['postTag', 'comment_body', 'comment_score', 'verified', 'is_gold', 'has_verified_email',\n",
    "       'link_karma', 'total_karma', 'created_utc', 'comment_karma']\n",
    "y = \"mod_deleted\"\n",
    "x_cont = x[2:]\n",
    "x_text = 'comment_body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   comment_score  verified  is_gold  has_verified_email  link_karma  \\\n",
      "0      28.597130       0.0 -0.32277            0.455035   -0.084809   \n",
      "1       7.747394       0.0 -0.32277            0.455035   -0.071857   \n",
      "2       4.718825       0.0 -0.32277            0.455035   -0.128258   \n",
      "3       2.339236       0.0 -0.32277            0.455035    0.022666   \n",
      "4       0.917663       0.0 -0.32277            0.455035    0.022504   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  actually      also  ...     still  \\\n",
      "0    -0.078767    -0.402122      -0.058956       0.0  0.000000  ...  0.000000   \n",
      "1     0.173269    -2.105970       0.260526       0.0  0.000000  ...  1.000000   \n",
      "2    -0.236544     0.515729      -0.246319       0.0  0.000000  ...  0.000000   \n",
      "3     1.475998     0.526639       1.973054       0.0  0.000000  ...  0.000000   \n",
      "4     0.102184    -1.309254       0.126044       0.0  0.338941  ...  0.667799   \n",
      "\n",
      "   take  things  think  time  want  way  white  would       ye  \n",
      "0   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.56772  \n",
      "1   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "2   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "3   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "4   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "#y is our target, what we are trying to predict. That is either deleted by mod or deleted by user (if deleted at all). Two splits.\n",
    "for df in dataframes:\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    # Use grouped split so that comments from a posts are not split between training & test set\n",
    "    X = df[x]\n",
    "    Y = df[y]\n",
    "    gs = GroupShuffleSplit(n_splits=2, test_size=.3, random_state=0)\n",
    "    train_ind, test_ind = next(gs.split(X, Y, groups=X.postTag))\n",
    "    X_train = X.iloc[train_ind].drop(\"postTag\", axis=1)\n",
    "    y_train = Y.iloc[train_ind]\n",
    "    X_test = X.iloc[test_ind].drop(\"postTag\", axis=1)\n",
    "    y_test = Y.iloc[test_ind]\n",
    "    \n",
    "    \"\"\"\n",
    "        Preprocess continuous columns and comment body\n",
    "        We may want to grid search with tfidf params instead of using above params\n",
    "    \"\"\"\n",
    "    cont_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    text_pipe = Pipeline([\n",
    "        ('vect', tfidf)\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cont', cont_pipe, x_cont),\n",
    "        ('text', text_pipe, x_text)\n",
    "    ])\n",
    "    \n",
    "    preprocessor.fit(X_train)\n",
    "    columns = preprocessor.named_transformers_[\"text\"][0].get_feature_names_out()\n",
    "    columns = list(x_cont) + list(columns)\n",
    "    X_train_trans = pd.DataFrame(preprocessor.transform(X_train).toarray(), columns=columns)\n",
    "    print(X_train_trans.head()) # Transformed training data\n",
    "    \n",
    "    # Create model pipeline & param_grids\n",
    "    ridge_params = {'ridge__alpha': [0, 0.5, 0.1, 0.01, 0.001, 1],  \n",
    "                    \"ridge__solver\": ['svd', 'cholesky', 'lsqr', 'sag', 'saga']}\n",
    "    ridge_pipe = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('ridge', Ridge())\n",
    "    ])\n",
    "    ridgeRegression = GridSearchCV(estimator=ridge_pipe, param_grid=ridge_params, return_train_score=True, cv=StratifiedKFold(), n_jobs=-1)\n",
    "    ridgeRegression.fit(X_train, y_train)\n",
    "    ridgeRegression.best_params_\n",
    "    \n",
    "    print(f\"Best Training Score: {ridgeRegression.best_score_}, Best PArams: {grid.best_params_}\")\n",
    "    print(f\"Test Score: {ridgeRegression.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
