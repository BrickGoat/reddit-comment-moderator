{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "subreddits = [\"kanye\", \"askScience\", \"PoliticalDiscussion\", \"socialism\"]\n",
    "\n",
    "#comments are pulled from the top 40 posts from the past month in each subreddit.\n",
    "dataframes = []\n",
    "for sub in subreddits:\n",
    "    df = pd.read_csv(f\"data/comments_{sub}.csv\")\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['postTag', 'user', 'comment_score', 'comment_body', 'mod_deleted',\n",
      "       'user_deleted', 'verified', 'is_gold', 'has_verified_email',\n",
      "       'link_karma', 'total_karma', 'created_utc', 'comment_karma'],\n",
      "      dtype='object')\n",
      "kanye\n",
      "(8821, 13)\n",
      "askScience\n",
      "(11134, 13)\n",
      "PoliticalDiscussion\n",
      "(31235, 13)\n",
      "socialism\n",
      "(3038, 13)\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].columns)\n",
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "(8384, 13)\n",
      "askScience\n",
      "(6679, 13)\n",
      "PoliticalDiscussion\n",
      "(28076, 13)\n",
      "socialism\n",
      "(2716, 13)\n"
     ]
    }
   ],
   "source": [
    "# Drop comments that were removed too quickly and were not archived\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].dropna(subset=[\"user\"])\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target is very uneven so we need to make sure we don't overfit on the majority class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "mod_deleted\n",
      "0    8243\n",
      "1     141\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    8147\n",
      "1     237\n",
      "dtype: int64\n",
      "\n",
      "askScience\n",
      "mod_deleted\n",
      "0    5146\n",
      "1    1533\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    6570\n",
      "1     109\n",
      "dtype: int64\n",
      "\n",
      "PoliticalDiscussion\n",
      "mod_deleted\n",
      "0    27239\n",
      "1      837\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    27411\n",
      "1      665\n",
      "dtype: int64\n",
      "\n",
      "socialism\n",
      "mod_deleted\n",
      "0.0    2487\n",
      "1.0     229\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0.0    2655\n",
      "1.0      61\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].groupby([\"mod_deleted\"]).size())\n",
    "    print(dataframes[i].groupby([\"user_deleted\"]).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postTag                object\n",
      "user                   object\n",
      "comment_score           int64\n",
      "comment_body           object\n",
      "mod_deleted             int64\n",
      "user_deleted            int64\n",
      "verified               object\n",
      "is_gold                object\n",
      "has_verified_email     object\n",
      "link_karma            float64\n",
      "total_karma           float64\n",
      "created_utc           float64\n",
      "comment_karma         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    There are a lot of ways you can estimate the p...\n",
       "1    Fisheries scientist cosigning. They may also t...\n",
       "2    To the fisheries scientists - any thought that...\n",
       "3    not a fishery scientist, but I recently transl...\n",
       "4    Here in New Zealand, we've had marine heat wav...\n",
       "Name: comment_body, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex expression for parsing escape sequences, or other invalid characters in the comment_body.\n",
    "#we are using the comment body to identify keywords, so main goal of the comment cleaning is just seperating the bodies into lists of words.\n",
    "print(kanyeData.dtypes) #->most values are numbers or objects. convert comment objects to strings to split into a list of keywords?\n",
    "\n",
    "kanyeData['comment_body'] = kanyeData['comment_body'].str.split()\n",
    "scienceData['comment_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "['album' 'black' 'even' 'get' 'good' 'jewish' 'jews' 'kanye' 'know' 'like'\n",
      " 'make' 'man' 'music' 'one' 'people' 'really' 'said' 'say' 'saying' 'see'\n",
      " 'shit' 'still' 'think' 'would' 'ye']\n",
      "\n",
      "askScience\n",
      "['also' 'body' 'cells' 'could' 'different' 'earth' 'even' 'get' 'know'\n",
      " 'like' 'lot' 'many' 'much' 'one' 'people' 'pressure' 'really' 'still'\n",
      " 'system' 'think' 'time' 'water' 'way' 'would' 'years']\n",
      "\n",
      "PoliticalDiscussion\n",
      "['also' 'democrats' 'election' 'even' 'get' 'going' 'know' 'like' 'make'\n",
      " 'much' 'one' 'party' 'people' 'really' 'republican' 'republicans' 'right'\n",
      " 'think' 'time' 'trump' 'us' 'vote' 'want' 'way' 'would']\n",
      "\n",
      "socialism\n",
      "['also' 'anti' 'even' 'get' 'good' 'including' 'know' 'like' 'make' 'one'\n",
      " 'people' 'please' 'really' 'right' 'see' 'social' 'socialism' 'socialist'\n",
      " 'socialists' 'think' 'time' 'us' 'well' 'world' 'would']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "comments_trans = []\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    comments = dataframes[i][\"comment_body\"]\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'), max_features=25)\n",
    "    vect.fit(comments)\n",
    "    print(subreddits[i])\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n",
    "    comments_trans.append(vect.transform(comments).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.76848418],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
