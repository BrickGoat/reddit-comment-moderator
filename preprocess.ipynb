{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "subreddits = [\"kanye\", \"askScience\", \"PoliticalDiscussion\", \"socialism\"]\n",
    "\n",
    "#comments are pulled from the top 40 posts from the past month in each subreddit.\n",
    "dataframes = []\n",
    "for sub in subreddits:\n",
    "    df = pd.read_csv(f\"data/comments_{sub}.csv\")\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['postTag', 'user', 'comment_score', 'comment_body', 'mod_deleted',\n",
      "       'user_deleted', 'verified', 'is_gold', 'has_verified_email',\n",
      "       'link_karma', 'total_karma', 'created_utc', 'comment_karma'],\n",
      "      dtype='object')\n",
      "kanye\n",
      "(42654, 13)\n",
      "askScience\n",
      "(11134, 13)\n",
      "PoliticalDiscussion\n",
      "(31235, 13)\n",
      "socialism\n",
      "(3038, 13)\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].columns)\n",
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop comments that were removed too quickly and so were not archived. Also drop comments automatically generated by mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "(40436, 13)\n",
      "askScience\n",
      "(6679, 13)\n",
      "PoliticalDiscussion\n",
      "(28026, 13)\n",
      "socialism\n",
      "(2620, 13)\n"
     ]
    }
   ],
   "source": [
    "urlRegex = r\"(https? *:*\\/*\\/*)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\"\n",
    "mods = [\"https://www.reddit.com/user/AutoModerator\", \"https://www.reddit.com/user/socialism-ModTeam\"]\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].dropna(subset=[\"user\"])\n",
    "    dataframes[i] = dataframes[i][~dataframes[i][\"user\"].isin(mods)]\n",
    "    for col in ['verified', 'is_gold', 'has_verified_email']:\n",
    "        dataframes[i][col] = dataframes[i][col].apply(lambda x: 1 if x else 0)\n",
    "    dataframes[i]['comment_body'] = dataframes[i]['comment_body'].apply(lambda x: re.sub(urlRegex, ' ', x))\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target is very uneven so we need to make sure we don't overfit on the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "mod_deleted\n",
      "0    39584\n",
      "1      852\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    38187\n",
      "1     2249\n",
      "dtype: int64\n",
      "\n",
      "askScience\n",
      "mod_deleted\n",
      "0    5146\n",
      "1    1533\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    6570\n",
      "1     109\n",
      "dtype: int64\n",
      "\n",
      "PoliticalDiscussion\n",
      "mod_deleted\n",
      "0    27189\n",
      "1      837\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    27361\n",
      "1      665\n",
      "dtype: int64\n",
      "\n",
      "socialism\n",
      "mod_deleted\n",
      "0    2392\n",
      "1     228\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    2559\n",
      "1      61\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].groupby([\"mod_deleted\"]).size())\n",
    "    print(dataframes[i].groupby([\"user_deleted\"]).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "['actual' 'also' 'ani' 'anti' 'becaus' 'black' 'call' 'even' 'fuck' 'get'\n",
      " 'go' 'good' 'hate' 'jew' 'jewish' 'kany' 'know' 'like' 'lol' 'make' 'man'\n",
      " 'mean' 'music' 'need' 'one' 'onli' 'peopl' 'person' 'point' 'realli'\n",
      " 'right' 'said' 'say' 'see' 'shit' 'someon' 'still' 'take' 'talk' 'thing'\n",
      " 'think' 'time' 'tri' 'use' 'want' 'way' 'whi' 'white' 'would' 'ye']\n",
      "\n",
      "['act like' 'anti semit' 'black peopl' 'black people' 'death con'\n",
      " 'eric andr' 'feel like' 'georg floyd' 'get help' 'jewish peopl'\n",
      " 'jewish people' 'kany said' 'kany west' 'like kany' 'look like'\n",
      " 'mental health' 'mental ill' 'need help' 'peopl like' 'piec shit'\n",
      " 'seem like' 'social media' 'sound like' 'white peopl' 'year old']\n",
      "\n",
      "askScience\n",
      "['actual' 'also' 'ani' 'becaus' 'bodi' 'caus' 'cell' 'could' 'differ'\n",
      " 'doe' 'earth' 'effect' 'enough' 'even' 'get' 'go' 'human' 'know' 'like'\n",
      " 'long' 'look' 'lot' 'make' 'mani' 'mean' 'much' 'need' 'one' 'onli'\n",
      " 'peopl' 'pressur' 'realli' 'say' 'see' 'someth' 'still' 'system' 'take'\n",
      " 'thing' 'think' 'time' 'use' 'veri' 'water' 'way' 'well' 'whi' 'work'\n",
      " 'would' 'year']\n",
      "\n",
      "['black hole' 'blood cell' 'dark matter' 'et al' 'event horizon'\n",
      " 'everi day' 'feel like' 'immun respons' 'immun system' 'long term'\n",
      " 'long time' 'look like' 'lose weight' 'million year' 'neutron star'\n",
      " 'pretti much' 'seem like' 'solar system' 'someth like' 'sound like'\n",
      " 'thing like' 'water pressur' 'water tower' 'would take' 'year ago']\n",
      "\n",
      "PoliticalDiscussion\n",
      "['actual' 'also' 'ani' 'becaus' 'biden' 'could' 'crime' 'democrat' 'elect'\n",
      " 'even' 'get' 'go' 'good' 'gop' 'know' 'like' 'make' 'mani' 'mean' 'much'\n",
      " 'need' 'one' 'onli' 'parti' 'peopl' 'point' 'polit' 'realli' 'republican'\n",
      " 'right' 'say' 'see' 'state' 'still' 'take' 'thing' 'think' 'time' 'tri'\n",
      " 'trump' 'us' 'use' 'vote' 'voter' 'want' 'way' 'whi' 'work' 'would'\n",
      " 'year']\n",
      "\n",
      "['affirm action' 'even though' 'feel like' 'look like' 'lot peopl'\n",
      " 'mani peopl' 'onli one' 'peopl like' 'peopl vote' 'peopl want'\n",
      " 'popular vote' 'pretti much' 'republican parti' 'right wing' 'seem like'\n",
      " 'social media' 'sound like' 'student loan' 'suprem court' 'tax cut'\n",
      " 'thing like' 'vote democrat' 'vote republican' 'year ago' 'young peopl']\n",
      "\n",
      "socialism\n",
      "['also' 'american' 'ani' 'becaus' 'capit' 'class' 'countri' 'even' 'fuck'\n",
      " 'get' 'go' 'good' 'know' 'like' 'look' 'make' 'mean' 'much' 'need'\n",
      " 'never' 'one' 'onli' 'peopl' 'point' 'power' 'protest' 'realli' 'right'\n",
      " 'say' 'see' 'social' 'socialist' 'state' 'still' 'support' 'take' 'thing'\n",
      " 'think' 'time' 'us' 'use' 'veri' 'want' 'way' 'well' 'whi' 'work' 'world'\n",
      " 'would' 'year']\n",
      "\n",
      "['charli kirk' 'civil war' 'communist parti' 'follow letter' 'free speech'\n",
      " 'left wing' 'letter follow' 'like say' 'look like' 'lot peopl'\n",
      " 'make sure' 'mani peopl' 'onli one' 'peopl like' 'right wing'\n",
      " 'rule class' 'seem like' 'sound like' 'south asian' 'think would'\n",
      " 'unit state' 'unit states' 'work class' 'year ago' 'year old']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "comments_trans = []\n",
    "\n",
    "# Added stemming to vectorization but may not be necessary, max_df has a bigger impact\n",
    "porter = SnowballStemmer(\"english\")\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    comments = dataframes[i][\"comment_body\"].apply(lambda x: \" \".join(tokenizer_porter(x)))\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(subreddits[i])\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n",
    "    comments_trans.append(vect.transform(comments).toarray())\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(2,2) ,max_features=25, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kanyeData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer, TfidfTransformer\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m make_pipeline\n\u001b[1;32m----> 5\u001b[0m kanye_tfidf \u001b[39m=\u001b[39m make_pipeline(CountVectorizer(stop_words\u001b[39m=\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m), TfidfTransformer())\u001b[39m.\u001b[39mfit_transform(kanyeData[\u001b[39m'\u001b[39m\u001b[39mcomment_body\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x)))\n\u001b[0;32m      6\u001b[0m kanye_tfidf\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kanyeData' is not defined"
     ]
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "x = ['postTag', 'comment_body', 'comment_score', 'verified', 'is_gold', 'has_verified_email',\n",
    "       'link_karma', 'total_karma', 'created_utc', 'comment_karma']\n",
    "y = \"mod_deleted\"\n",
    "x_cont = x[2:]\n",
    "x_text = 'comment_body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   comment_score  verified  is_gold  has_verified_email  link_karma  \\\n",
      "0      28.597130       0.0 -0.32277            0.455035   -0.084809   \n",
      "1       7.747394       0.0 -0.32277            0.455035   -0.071857   \n",
      "2       4.718825       0.0 -0.32277            0.455035   -0.128258   \n",
      "3       2.339236       0.0 -0.32277            0.455035    0.022666   \n",
      "4       0.917663       0.0 -0.32277            0.455035    0.022504   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  actually      also  ...     still  \\\n",
      "0    -0.078767    -0.402122      -0.058956       0.0  0.000000  ...  0.000000   \n",
      "1     0.173269    -2.105970       0.260526       0.0  0.000000  ...  1.000000   \n",
      "2    -0.236544     0.515729      -0.246319       0.0  0.000000  ...  0.000000   \n",
      "3     1.475998     0.526639       1.973054       0.0  0.000000  ...  0.000000   \n",
      "4     0.102184    -1.309254       0.126044       0.0  0.338941  ...  0.667799   \n",
      "\n",
      "   take  things  think  time  want  way  white  would       ye  \n",
      "0   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.56772  \n",
      "1   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "2   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "3   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "4   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'alpha' for estimator Pipeline(steps=[('vect',\n                 TfidfVectorizer(max_df=0.5, max_features=50,\n                                 ngram_range=(1, 2),\n                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n                                             'our', 'ours', 'ourselves', 'you',\n                                             \"you're\", \"you've\", \"you'll\",\n                                             \"you'd\", 'your', 'yours',\n                                             'yourself', 'yourselves', 'he',\n                                             'him', 'his', 'himself', 'she',\n                                             \"she's\", 'her', 'hers', 'herself',\n                                             'it', \"it's\", 'its', 'itself', ...],\n                                 strip_accents='unicode')),\n                ('clf', Ridge())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 674, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 188, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\metaestimators.py\", line 72, in _set_params\n    super().set_params(**params)\n  File \"C:\\Users\\commack\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 246, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'alpha' for estimator Pipeline(steps=[('vect',\n                 TfidfVectorizer(max_df=0.5, max_features=50,\n                                 ngram_range=(1, 2),\n                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n                                             'our', 'ours', 'ourselves', 'you',\n                                             \"you're\", \"you've\", \"you'll\",\n                                             \"you'd\", 'your', 'yours',\n                                             'yourself', 'yourselves', 'he',\n                                             'him', 'his', 'himself', 'she',\n                                             \"she's\", 'her', 'hers', 'herself',\n                                             'it', \"it's\", 'its', 'itself', ...],\n                                 strip_accents='unicode')),\n                ('clf', Ridge())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m ridge_pipe \u001b[39m=\u001b[39m Pipeline(steps\u001b[39m=\u001b[39m[(\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, tfidf),\n\u001b[0;32m     43\u001b[0m                              (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, rr)])\n\u001b[0;32m     44\u001b[0m ridgeRegression \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mridge_pipe, param_grid\u001b[39m=\u001b[39mridge_params, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cv\u001b[39m=\u001b[39mStratifiedKFold(), n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m ridgeRegression\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     46\u001b[0m ridgeRegression\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest Training Score: \u001b[39m\u001b[39m{\u001b[39;00mridgeRegression\u001b[39m.\u001b[39mbest_score_\u001b[39m}\u001b[39;00m\u001b[39m, Best PArams: \u001b[39m\u001b[39m{\u001b[39;00mgrid\u001b[39m.\u001b[39mbest_params_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2288.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2288.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'alpha' for estimator Pipeline(steps=[('vect',\n                 TfidfVectorizer(max_df=0.5, max_features=50,\n                                 ngram_range=(1, 2),\n                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n                                             'our', 'ours', 'ourselves', 'you',\n                                             \"you're\", \"you've\", \"you'll\",\n                                             \"you'd\", 'your', 'yours',\n                                             'yourself', 'yourselves', 'he',\n                                             'him', 'his', 'himself', 'she',\n                                             \"she's\", 'her', 'hers', 'herself',\n                                             'it', \"it's\", 'its', 'itself', ...],\n                                 strip_accents='unicode')),\n                ('clf', Ridge())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "#y is our target, what we are trying to predict. That is either deleted by mod or deleted by user (if deleted at all). Two splits.\n",
    "for df in dataframes:\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    # Use grouped split so that comments from a posts are not split between training & test set\n",
    "    X = df[x]\n",
    "    Y = df[y]\n",
    "    gs = GroupShuffleSplit(n_splits=2, test_size=.3, random_state=0)\n",
    "    train_ind, test_ind = next(gs.split(X, Y, groups=X.postTag))\n",
    "    X_train = X.iloc[train_ind].drop(\"postTag\", axis=1)\n",
    "    y_train = Y.iloc[train_ind]\n",
    "    X_test = X.iloc[test_ind].drop(\"postTag\", axis=1)\n",
    "    y_test = Y.iloc[test_ind]\n",
    "    \n",
    "    \"\"\"\n",
    "        Preprocess continuous columns and comment body\n",
    "        We may want to grid search with tfidf params instead of using above params\n",
    "    \"\"\"\n",
    "    cont_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    text_pipe = Pipeline([\n",
    "        ('vect', tfidf)\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cont', cont_pipe, x_cont),\n",
    "        ('text', text_pipe, x_text)\n",
    "    ])\n",
    "    \n",
    "    preprocessor.fit(X_train)\n",
    "    columns = preprocessor.named_transformers_[\"text\"][0].get_feature_names_out()\n",
    "    columns = list(x_cont) + list(columns)\n",
    "    X_train_trans = pd.DataFrame(preprocessor.transform(X_train).toarray(), columns=columns)\n",
    "    print(X_train_trans.head()) # Transformed training data\n",
    "    \n",
    "    # Create model pipeline & param_grids\n",
    "    ridge_params = {'alpha': [0, 0.5, 0.1, 0.01, 0.001, 1],  \n",
    "                  \"solver\": ['svd', 'cholesky', 'lsqr', 'sag', 'saga']}\n",
    "    rr = Ridge()\n",
    "    ridge_pipe = Pipeline(steps=[('vect', tfidf),\n",
    "                                 ('clf', rr)])\n",
    "    ridgeRegression = GridSearchCV(estimator=ridge_pipe, param_grid=ridge_params, return_train_score=True, cv=StratifiedKFold(), n_jobs=-1)\n",
    "    ridgeRegression.fit(X_train, y_train)\n",
    "    ridgeRegression.best_params_\n",
    "    \n",
    "    print(f\"Best Training Score: {ridgeRegression.best_score_}, Best PArams: {grid.best_params_}\")\n",
    "    print(f\"Test Score: {ridgeRegression.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
