{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "subreddits = [\"kanye\", \"askScience\", \"PoliticalDiscussion\", \"socialism\"]\n",
    "\n",
    "#comments are pulled from the top 40 posts from the past month in each subreddit.\n",
    "dataframes = []\n",
    "for sub in subreddits:\n",
    "    df = pd.read_csv(f\"data/comments_{sub}.csv\")\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['postTag', 'user', 'comment_score', 'comment_body', 'mod_deleted',\n",
      "       'user_deleted', 'verified', 'is_gold', 'has_verified_email',\n",
      "       'link_karma', 'total_karma', 'created_utc', 'comment_karma'],\n",
      "      dtype='object')\n",
      "kanye\n",
      "(42654, 13)\n",
      "askScience\n",
      "(11134, 13)\n",
      "PoliticalDiscussion\n",
      "(31235, 13)\n",
      "socialism\n",
      "(3038, 13)\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].columns)\n",
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop comments that were removed too quickly and so were not archived. Also drop comments automatically generated by mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "(40436, 13)\n",
      "askScience\n",
      "(6679, 13)\n",
      "PoliticalDiscussion\n",
      "(28026, 13)\n",
      "socialism\n",
      "(2620, 13)\n"
     ]
    }
   ],
   "source": [
    "mods = [\"https://www.reddit.com/user/AutoModerator\", \"https://www.reddit.com/user/socialism-ModTeam\"]\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].dropna(subset=[\"user\"])\n",
    "    dataframes[i] = dataframes[i][~dataframes[i][\"user\"].isin(mods)]\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target is very uneven so we need to make sure we don't overfit on the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "mod_deleted\n",
      "0    8243\n",
      "1     141\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    8147\n",
      "1     237\n",
      "dtype: int64\n",
      "\n",
      "askScience\n",
      "mod_deleted\n",
      "0    5146\n",
      "1    1533\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    6570\n",
      "1     109\n",
      "dtype: int64\n",
      "\n",
      "PoliticalDiscussion\n",
      "mod_deleted\n",
      "0    27239\n",
      "1      837\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    27411\n",
      "1      665\n",
      "dtype: int64\n",
      "\n",
      "socialism\n",
      "mod_deleted\n",
      "0.0    2487\n",
      "1.0     229\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0.0    2655\n",
      "1.0      61\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].groupby([\"mod_deleted\"]).size())\n",
    "    print(dataframes[i].groupby([\"user_deleted\"]).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "['actual' 'also' 'ani' 'anti' 'becaus' 'black' 'call' 'even' 'fuck' 'get'\n",
      " 'go' 'good' 'hate' 'jew' 'jewish' 'kany' 'know' 'like' 'lol' 'make' 'man'\n",
      " 'mean' 'music' 'need' 'one' 'onli' 'peopl' 'person' 'point' 'realli'\n",
      " 'right' 'said' 'say' 'see' 'shit' 'someon' 'still' 'take' 'talk' 'thing'\n",
      " 'think' 'time' 'tri' 'use' 'want' 'way' 'whi' 'white' 'would' 'ye']\n",
      "\n",
      "['act like' 'anti semit' 'black peopl' 'black people' 'eric andr'\n",
      " 'feel like' 'georg floyd' 'get help' 'https www' 'jewish peopl'\n",
      " 'jewish people' 'kany said' 'kany west' 'like kany' 'look like'\n",
      " 'mental health' 'mental ill' 'need help' 'peopl like' 'piec shit'\n",
      " 'seem like' 'social media' 'sound like' 'white peopl' 'year old']\n",
      "\n",
      "askScience\n",
      "['actual' 'also' 'ani' 'becaus' 'bodi' 'caus' 'cell' 'could' 'differ'\n",
      " 'doe' 'earth' 'effect' 'enough' 'even' 'get' 'go' 'human' 'know' 'like'\n",
      " 'long' 'look' 'lot' 'make' 'mani' 'mean' 'much' 'need' 'one' 'onli'\n",
      " 'peopl' 'pressur' 'realli' 'say' 'see' 'someth' 'still' 'system' 'take'\n",
      " 'thing' 'think' 'time' 'use' 'veri' 'water' 'way' 'well' 'whi' 'work'\n",
      " 'would' 'year']\n",
      "\n",
      "['black hole' 'dark matter' 'en wikipedia' 'et al' 'event horizon'\n",
      " 'feel like' 'https en' 'https www' 'immun system' 'long term' 'long time'\n",
      " 'look like' 'lose weight' 'million year' 'org wiki' 'pretti much'\n",
      " 'seem like' 'solar system' 'someth like' 'sound like' 'thing like'\n",
      " 'water pressur' 'water tower' 'wikipedia org' 'year ago']\n",
      "\n",
      "PoliticalDiscussion\n",
      "['actual' 'also' 'ani' 'becaus' 'biden' 'could' 'crime' 'democrat' 'elect'\n",
      " 'even' 'get' 'go' 'good' 'gop' 'know' 'like' 'make' 'mani' 'mean' 'much'\n",
      " 'need' 'one' 'onli' 'parti' 'peopl' 'point' 'polit' 'realli' 'republican'\n",
      " 'right' 'say' 'see' 'state' 'still' 'take' 'thing' 'think' 'time' 'tri'\n",
      " 'trump' 'us' 'use' 'vote' 'voter' 'want' 'way' 'whi' 'work' 'would'\n",
      " 'year']\n",
      "\n",
      "['affirm action' 'defund police' 'even though' 'feel like' 'https www'\n",
      " 'look like' 'lot peopl' 'mani peopl' 'peopl like' 'peopl vote'\n",
      " 'peopl want' 'pretti much' 'republican parti' 'right wing' 'seem like'\n",
      " 'social media' 'sound like' 'student loan' 'suprem court' 'tax cut'\n",
      " 'thing like' 'vote democrat' 'vote republican' 'year ago' 'young peopl']\n",
      "\n",
      "socialism\n",
      "['also' 'american' 'ani' 'becaus' 'capit' 'class' 'countri' 'even' 'fuck'\n",
      " 'get' 'go' 'good' 'know' 'like' 'look' 'make' 'mean' 'much' 'need'\n",
      " 'never' 'one' 'onli' 'peopl' 'point' 'power' 'protest' 'realli' 'right'\n",
      " 'say' 'see' 'social' 'socialist' 'state' 'still' 'support' 'take' 'thing'\n",
      " 'think' 'time' 'us' 'use' 'veri' 'want' 'way' 'well' 'whi' 'work' 'world'\n",
      " 'would' 'year']\n",
      "\n",
      "['charli kirk' 'civil war' 'communist parti' 'far right' 'follow letter'\n",
      " 'free speech' 'https www' 'https youtu' 'letter follow' 'look like'\n",
      " 'make sure' 'mani peopl' 'onli one' 'peopl like' 'right wing'\n",
      " 'rule class' 'seem like' 'sound like' 'south asian' 'think would'\n",
      " 'unit state' 'unit states' 'work class' 'year ago' 'year old']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "comments_trans = []\n",
    "\n",
    "# Added stemming to vectorization but may not be necessary, max_df has a bigger impact\n",
    "porter = SnowballStemmer(\"english\")\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    comments = dataframes[i][\"comment_body\"].apply(lambda x: \" \".join(tokenizer_porter(x)))\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(subreddits[i])\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n",
    "    comments_trans.append(vect.transform(comments).toarray())\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(2,2) ,max_features=25, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.76848418],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
