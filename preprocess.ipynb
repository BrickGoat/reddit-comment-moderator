{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postTag</th>\n",
       "      <th>user</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>mod_deleted</th>\n",
       "      <th>user_deleted</th>\n",
       "      <th>verified</th>\n",
       "      <th>is_gold</th>\n",
       "      <th>has_verified_email</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>total_karma</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>comment_karma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/HankScorpio42</td>\n",
       "      <td>903</td>\n",
       "      <td>There is this Stockholm syndrome when it comes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>407101.0</td>\n",
       "      <td>503481.0</td>\n",
       "      <td>1.462790e+09</td>\n",
       "      <td>90018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/jacquix</td>\n",
       "      <td>280</td>\n",
       "      <td>\\nThere is this Stockholm syndrome when it com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>558.0</td>\n",
       "      <td>9746.0</td>\n",
       "      <td>1.466185e+09</td>\n",
       "      <td>9140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/travissius</td>\n",
       "      <td>70</td>\n",
       "      <td>I hadn't noticed there was a comment section b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.612706e+09</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/Indoril_Nereguar</td>\n",
       "      <td>75</td>\n",
       "      <td>'It's not changed my opinion of her, I always ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>18262.0</td>\n",
       "      <td>61808.0</td>\n",
       "      <td>1.516709e+09</td>\n",
       "      <td>42347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/pdrock7</td>\n",
       "      <td>45</td>\n",
       "      <td>I mean i do too, but she inspires me to be vio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>65905.0</td>\n",
       "      <td>122831.0</td>\n",
       "      <td>1.343871e+09</td>\n",
       "      <td>55896.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  postTag                                          user  comment_score  \\\n",
       "0  ykylq2     https://www.reddit.com/user/HankScorpio42            903   \n",
       "1  ykylq2           https://www.reddit.com/user/jacquix            280   \n",
       "2  ykylq2        https://www.reddit.com/user/travissius             70   \n",
       "3  ykylq2  https://www.reddit.com/user/Indoril_Nereguar             75   \n",
       "4  ykylq2           https://www.reddit.com/user/pdrock7             45   \n",
       "\n",
       "                                        comment_body  mod_deleted  \\\n",
       "0  There is this Stockholm syndrome when it comes...            0   \n",
       "1  \\nThere is this Stockholm syndrome when it com...            0   \n",
       "2  I hadn't noticed there was a comment section b...            0   \n",
       "3  'It's not changed my opinion of her, I always ...            0   \n",
       "4  I mean i do too, but she inspires me to be vio...            0   \n",
       "\n",
       "   user_deleted verified is_gold has_verified_email  link_karma  total_karma  \\\n",
       "0             0     True   False               True    407101.0     503481.0   \n",
       "1             0     True   False               True       558.0       9746.0   \n",
       "2             0     True   False               True         1.0        240.0   \n",
       "3             0     True   False               True     18262.0      61808.0   \n",
       "4             0     True   False               True     65905.0     122831.0   \n",
       "\n",
       "    created_utc  comment_karma  \n",
       "0  1.462790e+09        90018.0  \n",
       "1  1.466185e+09         9140.0  \n",
       "2  1.612706e+09          231.0  \n",
       "3  1.516709e+09        42347.0  \n",
       "4  1.343871e+09        55896.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#comments full is pulled from the top 20 posts in kanye subreddit.\n",
    "kanyeData = pd.read_csv(\"data/comments_full.csv\", index_col=0)\n",
    "scienceData = pd.read_csv(\"data/comments_askScience.csv\")\n",
    "politicalData = pd.read_csv(\"data/comments_PoliticalDiscussion.csv\")\n",
    "socialismData = pd.read_csv(\"data/comments_socialism.csv\")\n",
    "\n",
    "socialismData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postTag                object\n",
      "user                   object\n",
      "comment_score           int64\n",
      "comment_body           object\n",
      "mod_deleted             int64\n",
      "user_deleted            int64\n",
      "verified               object\n",
      "is_gold                object\n",
      "has_verified_email     object\n",
      "link_karma            float64\n",
      "total_karma           float64\n",
      "created_utc           float64\n",
      "comment_karma         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [Also, who, is, this, fucking, interviewer, eg...\n",
       "1    [The, professional, paparazzi, literally, try,...\n",
       "2    [Theyâ€™re, trying, to, get, him, to, say, somet...\n",
       "3                 [Naw, Dawg, he, just, mentally, ill]\n",
       "4                                     [Why, not, both]\n",
       "Name: comment_body, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex expression for parsing escape sequences, or other invalid characters in the comment_body.\n",
    "#we are using the comment body to identify keywords, so main goal of the comment cleaning is just seperating the bodies into lists of words.\n",
    "print(kanyeData.dtypes) #->most values are numbers or objects. convert comment objects to strings to split into a list of keywords?\n",
    "\n",
    "kanyeData['comment_body'] = kanyeData['comment_body'].str.split()\n",
    "kanyeData['comment_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#each row in kanye_comments is a different bag of words.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#run bag of words through tokenizer, build a vocabulary over all document, and encode the matrix.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m vect \u001b[39m=\u001b[39m CountVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m vect\u001b[39m.\u001b[39;49mfit(kanye_comments)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(vect\u001b[39m.\u001b[39mget_feature_names())\n\u001b[0;32m     12\u001b[0m X \u001b[39m=\u001b[39m vect\u001b[39m.\u001b[39mtransform(kanye_comments)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:1291\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \n\u001b[0;32m   1277\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> 1291\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1292\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "kanye_comments = kanyeData['comment_body']\n",
    "#each row in kanye_comments is a different bag of words.\n",
    "#run bag of words through tokenizer, build a vocabulary over all document, and encode the matrix.\n",
    "vect = CountVectorizer(max_features=10)\n",
    "vect.fit(kanye_comments)\n",
    "print(vect.get_feature_names())\n",
    "\n",
    "X = vect.transform(kanye_comments)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(), TfidfTransformer()).fit_transform(kanyeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
