{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postTag</th>\n",
       "      <th>user</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>mod_deleted</th>\n",
       "      <th>user_deleted</th>\n",
       "      <th>verified</th>\n",
       "      <th>is_gold</th>\n",
       "      <th>has_verified_email</th>\n",
       "      <th>link_karma</th>\n",
       "      <th>total_karma</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>comment_karma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/HankScorpio42</td>\n",
       "      <td>903</td>\n",
       "      <td>There is this Stockholm syndrome when it comes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>407101.0</td>\n",
       "      <td>503481.0</td>\n",
       "      <td>1.462790e+09</td>\n",
       "      <td>90018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/jacquix</td>\n",
       "      <td>280</td>\n",
       "      <td>\\nThere is this Stockholm syndrome when it com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>558.0</td>\n",
       "      <td>9746.0</td>\n",
       "      <td>1.466185e+09</td>\n",
       "      <td>9140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/travissius</td>\n",
       "      <td>70</td>\n",
       "      <td>I hadn't noticed there was a comment section b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.612706e+09</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/Indoril_Nereguar</td>\n",
       "      <td>75</td>\n",
       "      <td>'It's not changed my opinion of her, I always ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>18262.0</td>\n",
       "      <td>61808.0</td>\n",
       "      <td>1.516709e+09</td>\n",
       "      <td>42347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ykylq2</td>\n",
       "      <td>https://www.reddit.com/user/pdrock7</td>\n",
       "      <td>45</td>\n",
       "      <td>I mean i do too, but she inspires me to be vio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>65905.0</td>\n",
       "      <td>122831.0</td>\n",
       "      <td>1.343871e+09</td>\n",
       "      <td>55896.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  postTag                                          user  comment_score  \\\n",
       "0  ykylq2     https://www.reddit.com/user/HankScorpio42            903   \n",
       "1  ykylq2           https://www.reddit.com/user/jacquix            280   \n",
       "2  ykylq2        https://www.reddit.com/user/travissius             70   \n",
       "3  ykylq2  https://www.reddit.com/user/Indoril_Nereguar             75   \n",
       "4  ykylq2           https://www.reddit.com/user/pdrock7             45   \n",
       "\n",
       "                                        comment_body  mod_deleted  \\\n",
       "0  There is this Stockholm syndrome when it comes...            0   \n",
       "1  \\nThere is this Stockholm syndrome when it com...            0   \n",
       "2  I hadn't noticed there was a comment section b...            0   \n",
       "3  'It's not changed my opinion of her, I always ...            0   \n",
       "4  I mean i do too, but she inspires me to be vio...            0   \n",
       "\n",
       "   user_deleted verified is_gold has_verified_email  link_karma  total_karma  \\\n",
       "0             0     True   False               True    407101.0     503481.0   \n",
       "1             0     True   False               True       558.0       9746.0   \n",
       "2             0     True   False               True         1.0        240.0   \n",
       "3             0     True   False               True     18262.0      61808.0   \n",
       "4             0     True   False               True     65905.0     122831.0   \n",
       "\n",
       "    created_utc  comment_karma  \n",
       "0  1.462790e+09        90018.0  \n",
       "1  1.466185e+09         9140.0  \n",
       "2  1.612706e+09          231.0  \n",
       "3  1.516709e+09        42347.0  \n",
       "4  1.343871e+09        55896.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#comments full is pulled from the top 20 posts in kanye subreddit.\n",
    "kanyeData = pd.read_csv(\"data/comments_full.csv\", index_col=0)\n",
    "scienceData = pd.read_csv(\"data/comments_askScience.csv\")\n",
    "politicalData = pd.read_csv(\"data/comments_PoliticalDiscussion.csv\")\n",
    "socialismData = pd.read_csv(\"data/comments_socialism.csv\")\n",
    "\n",
    "socialismData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postTag                object\n",
      "user                   object\n",
      "comment_score           int64\n",
      "comment_body           object\n",
      "mod_deleted             int64\n",
      "user_deleted            int64\n",
      "verified               object\n",
      "is_gold                object\n",
      "has_verified_email     object\n",
      "link_karma            float64\n",
      "total_karma           float64\n",
      "created_utc           float64\n",
      "comment_karma         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    There are a lot of ways you can estimate the p...\n",
       "1    Fisheries scientist cosigning. They may also t...\n",
       "2    To the fisheries scientists - any thought that...\n",
       "3    not a fishery scientist, but I recently transl...\n",
       "4    Here in New Zealand, we've had marine heat wav...\n",
       "Name: comment_body, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex expression for parsing escape sequences, or other invalid characters in the comment_body.\n",
    "#we are using the comment body to identify keywords, so main goal of the comment cleaning is just seperating the bodies into lists of words.\n",
    "print(kanyeData.dtypes) #->most values are numbers or objects. convert comment objects to strings to split into a list of keywords?\n",
    "\n",
    "kanyeData['comment_body'] = kanyeData['comment_body'].str.split()\n",
    "scienceData['comment_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black' 'even' 'get' 'jewish' 'jews' 'kanye' 'know' 'like' 'make' 'man'\n",
      " 'music' 'one' 'people' 'really' 'removed' 'said' 'say' 'saying' 'seconds'\n",
      " 'shit' 'still' 'think' 'within' 'would' 'ye']\n",
      "['and' 'are' 'as' 'be' 'but' 'can' 'for' 'have' 'in' 'is' 'it' 'not' 'of'\n",
      " 'on' 'or' 'removed' 'seconds' 'that' 'the' 'they' 'this' 'to' 'with'\n",
      " 'within' 'you']\n",
      "['also' 'democrats' 'election' 'even' 'get' 'going' 'like' 'make' 'much'\n",
      " 'one' 'party' 'people' 'removed' 'republican' 'republicans' 'right'\n",
      " 'seconds' 'think' 'time' 'trump' 'us' 'vote' 'want' 'within' 'would']\n",
      "['also' 'even' 'get' 'good' 'including' 'like' 'one' 'people' 'please'\n",
      " 'really' 'removed' 'right' 'seconds' 'see' 'social' 'socialism'\n",
      " 'socialist' 'socialists' 'think' 'time' 'us' 'well' 'within' 'world'\n",
      " 'would']\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "kanye_comments = kanyeData['comment_body'].apply(lambda x: \" \".join(x))\n",
    "#each row in kanye_comments is a different bag of words.\n",
    "kanyeVect = CountVectorizer(stop_words=stopwords.words('english'), max_features=25)\n",
    "kanyeVect.fit(kanye_comments)\n",
    "#print(kanyeVect.get_feature_names_out())\n",
    "kanyeX = kanyeVect.transform(kanye_comments).toarray()\n",
    "\n",
    "scienceVect = CountVectorizer(max_features=25)\n",
    "scienceVect.fit(scienceData['comment_body'])\n",
    "#print(scienceVect.get_feature_names_out())\n",
    "scienceX = scienceVect.transform(scienceData['comment_body']).toarray()\n",
    "\n",
    "politicalVect = CountVectorizer(stop_words=stopwords.words('english'), max_features=25)\n",
    "politicalVect.fit(politicalData['comment_body'])\n",
    "#print(politicalVect.get_feature_names_out())\n",
    "politicalX = politicalVect.transform(politicalData['comment_body']).toarray()\n",
    "\n",
    "socialismVect = CountVectorizer(stop_words=stopwords.words('english'), max_features=25)\n",
    "socialismVect.fit(socialismData['comment_body'])\n",
    "#print(socialismVect.get_feature_names_out())\n",
    "socialismX = socialismVect.transform(socialismData['comment_body']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.76848418],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
