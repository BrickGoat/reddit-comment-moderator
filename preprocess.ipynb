{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of preprocessing for the data we are using in class will be cleaning it for initial visualization. The comment bodies will need escape sequences removed, emojis/invalid characters parsed and removed, and any other issues in our data that could prevent a seamless exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package and data importing and loading\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "subreddits = [\"kanye\", \"askScience\", \"PoliticalDiscussion\", \"socialism\"]\n",
    "\n",
    "#comments are pulled from the top 40 posts from the past month in each subreddit.\n",
    "dataframes = []\n",
    "for sub in subreddits:\n",
    "    df = pd.read_csv(f\"data/comments_{sub}.csv\")\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['postTag', 'user', 'comment_score', 'comment_body', 'mod_deleted',\n",
      "       'user_deleted', 'verified', 'is_gold', 'has_verified_email',\n",
      "       'link_karma', 'total_karma', 'created_utc', 'comment_karma'],\n",
      "      dtype='object')\n",
      "kanye\n",
      "(42654, 13)\n",
      "askScience\n",
      "(11134, 13)\n",
      "PoliticalDiscussion\n",
      "(31235, 13)\n",
      "socialism\n",
      "(3038, 13)\n"
     ]
    }
   ],
   "source": [
    "print(dataframes[0].columns)\n",
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop comments that were removed too quickly and so were not archived. Also drop comments automatically generated by mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "(40436, 13)\n",
      "askScience\n",
      "(6679, 13)\n",
      "PoliticalDiscussion\n",
      "(28026, 13)\n",
      "socialism\n",
      "(2620, 13)\n"
     ]
    }
   ],
   "source": [
    "urlRegex = r\"(https? *:*\\/*\\/*)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\"\n",
    "mods = [\"https://www.reddit.com/user/AutoModerator\", \"https://www.reddit.com/user/socialism-ModTeam\"]\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].dropna(subset=[\"user\"])\n",
    "    dataframes[i] = dataframes[i][~dataframes[i][\"user\"].isin(mods)]\n",
    "    for col in ['verified', 'is_gold', 'has_verified_email']:\n",
    "        dataframes[i][col] = dataframes[i][col].apply(lambda x: 1 if x else 0)\n",
    "    dataframes[i]['comment_body'] = dataframes[i]['comment_body'].apply(lambda x: re.sub(urlRegex, ' ', x))\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target is very uneven so we need to make sure we don't overfit on the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "mod_deleted\n",
      "0    39584\n",
      "1      852\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    38187\n",
      "1     2249\n",
      "dtype: int64\n",
      "\n",
      "askScience\n",
      "mod_deleted\n",
      "0    5146\n",
      "1    1533\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    6570\n",
      "1     109\n",
      "dtype: int64\n",
      "\n",
      "PoliticalDiscussion\n",
      "mod_deleted\n",
      "0    27189\n",
      "1      837\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    27361\n",
      "1      665\n",
      "dtype: int64\n",
      "\n",
      "socialism\n",
      "mod_deleted\n",
      "0    2392\n",
      "1     228\n",
      "dtype: int64\n",
      "user_deleted\n",
      "0    2559\n",
      "1      61\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    print(subreddits[i])\n",
    "    print(dataframes[i].groupby([\"mod_deleted\"]).size())\n",
    "    print(dataframes[i].groupby([\"user_deleted\"]).size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning escape sequences, invalid words, deleted comments, and other things that won't serve to help our analysis. regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be looking at which variables (from the data we collected) are the most useful in classifying whether a comment gets deleted, and if so, whether the user deleted it themselves, or a moderator deleted it. Can we predict based on certain keywords, or a threshold for karma, or any other classifers, what the outcome of the comments status will be? Could this information we use be utilized to enhance the auto moderator currently used on reddit?\n",
    "\n",
    "The main classifier/variable we are studying will obviously be the comment bodies, as that content will be most critical to parsing the synoposis of messages that routinely get deleted or not. Thus, the data will be mostly free text, with no predefined features. As such, we will use multiple techniques to create training data to be used in model selection and training. Correlations discovered between account creation, comment karma, will be observed but will require less cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kanye\n",
      "['actual' 'also' 'ani' 'anti' 'becaus' 'black' 'call' 'even' 'fuck' 'get'\n",
      " 'go' 'good' 'hate' 'jew' 'jewish' 'kany' 'know' 'like' 'lol' 'make' 'man'\n",
      " 'mean' 'music' 'need' 'one' 'onli' 'peopl' 'person' 'point' 'realli'\n",
      " 'right' 'said' 'say' 'see' 'shit' 'someon' 'still' 'take' 'talk' 'thing'\n",
      " 'think' 'time' 'tri' 'use' 'want' 'way' 'whi' 'white' 'would' 'ye']\n",
      "\n",
      "['act like' 'anti semit' 'black peopl' 'black people' 'death con'\n",
      " 'eric andr' 'feel like' 'georg floyd' 'get help' 'jewish peopl'\n",
      " 'jewish people' 'kany said' 'kany west' 'like kany' 'look like'\n",
      " 'mental health' 'mental ill' 'need help' 'peopl like' 'piec shit'\n",
      " 'seem like' 'social media' 'sound like' 'white peopl' 'year old']\n",
      "\n",
      "askScience\n",
      "['actual' 'also' 'ani' 'becaus' 'bodi' 'caus' 'cell' 'could' 'differ'\n",
      " 'doe' 'earth' 'effect' 'enough' 'even' 'get' 'go' 'human' 'know' 'like'\n",
      " 'long' 'look' 'lot' 'make' 'mani' 'mean' 'much' 'need' 'one' 'onli'\n",
      " 'peopl' 'pressur' 'realli' 'say' 'see' 'someth' 'still' 'system' 'take'\n",
      " 'thing' 'think' 'time' 'use' 'veri' 'water' 'way' 'well' 'whi' 'work'\n",
      " 'would' 'year']\n",
      "\n",
      "['black hole' 'blood cell' 'dark matter' 'et al' 'event horizon'\n",
      " 'everi day' 'feel like' 'immun respons' 'immun system' 'long term'\n",
      " 'long time' 'look like' 'lose weight' 'million year' 'neutron star'\n",
      " 'pretti much' 'seem like' 'solar system' 'someth like' 'sound like'\n",
      " 'thing like' 'water pressur' 'water tower' 'would take' 'year ago']\n",
      "\n",
      "PoliticalDiscussion\n",
      "['actual' 'also' 'ani' 'becaus' 'biden' 'could' 'crime' 'democrat' 'elect'\n",
      " 'even' 'get' 'go' 'good' 'gop' 'know' 'like' 'make' 'mani' 'mean' 'much'\n",
      " 'need' 'one' 'onli' 'parti' 'peopl' 'point' 'polit' 'realli' 'republican'\n",
      " 'right' 'say' 'see' 'state' 'still' 'take' 'thing' 'think' 'time' 'tri'\n",
      " 'trump' 'us' 'use' 'vote' 'voter' 'want' 'way' 'whi' 'work' 'would'\n",
      " 'year']\n",
      "\n",
      "['affirm action' 'even though' 'feel like' 'look like' 'lot peopl'\n",
      " 'mani peopl' 'onli one' 'peopl like' 'peopl vote' 'peopl want'\n",
      " 'popular vote' 'pretti much' 'republican parti' 'right wing' 'seem like'\n",
      " 'social media' 'sound like' 'student loan' 'suprem court' 'tax cut'\n",
      " 'thing like' 'vote democrat' 'vote republican' 'year ago' 'young peopl']\n",
      "\n",
      "socialism\n",
      "['also' 'american' 'ani' 'becaus' 'capit' 'class' 'countri' 'even' 'fuck'\n",
      " 'get' 'go' 'good' 'know' 'like' 'look' 'make' 'mean' 'much' 'need'\n",
      " 'never' 'one' 'onli' 'peopl' 'point' 'power' 'protest' 'realli' 'right'\n",
      " 'say' 'see' 'social' 'socialist' 'state' 'still' 'support' 'take' 'thing'\n",
      " 'think' 'time' 'us' 'use' 'veri' 'want' 'way' 'well' 'whi' 'work' 'world'\n",
      " 'would' 'year']\n",
      "\n",
      "['charli kirk' 'civil war' 'communist parti' 'follow letter' 'free speech'\n",
      " 'left wing' 'letter follow' 'like say' 'look like' 'lot peopl'\n",
      " 'make sure' 'mani peopl' 'onli one' 'peopl like' 'right wing'\n",
      " 'rule class' 'seem like' 'sound like' 'south asian' 'think would'\n",
      " 'unit state' 'unit states' 'work class' 'year ago' 'year old']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer \n",
    "#We will use CountVectorizer during vectorization of datasets.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "comments_trans = []\n",
    "\n",
    "# Added stemming to vectorization but may not be necessary, max_df has a bigger impact\n",
    "porter = SnowballStemmer(\"english\")\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    comments = dataframes[i][\"comment_body\"].apply(lambda x: \" \".join(tokenizer_porter(x)))\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(subreddits[i])\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n",
    "    comments_trans.append(vect.transform(comments).toarray())\n",
    "    vect = CountVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(2,2) ,max_features=25, strip_accents=\"unicode\")\n",
    "    vect.fit(comments)\n",
    "    print(vect.get_feature_names_out())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kanyeData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer, TfidfTransformer\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m make_pipeline\n\u001b[1;32m----> 5\u001b[0m kanye_tfidf \u001b[39m=\u001b[39m make_pipeline(CountVectorizer(stop_words\u001b[39m=\u001b[39mstopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m), max_features\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m), TfidfTransformer())\u001b[39m.\u001b[39mfit_transform(kanyeData[\u001b[39m'\u001b[39m\u001b[39mcomment_body\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(x)))\n\u001b[0;32m      6\u001b[0m kanye_tfidf\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kanyeData' is not defined"
     ]
    }
   ],
   "source": [
    "#TF-IDF Rescaling Calculations. -> [Utilizing a param grid or pipeline could simplify this process.]\n",
    "#-> A statistical measure to evaluate how relevant a word is to a document.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kanye_tfidf = make_pipeline(CountVectorizer(stop_words=stopwords.words('english'), max_features=10), TfidfTransformer()).fit_transform(kanyeData['comment_body'].apply(lambda x: \" \".join(x)))\n",
    "kanye_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {\"countvectorizer__ngram_range\":[(1, 2), (2, 5)],\n",
    "              \"countvectorizer__min_df\": [2, 3]\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(CountVectorizer(analyzer=\"char\"), LogisticRegression()), param_grid=param_grid,\n",
    "                                  cv=10, scoring=\"f1_macro\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "x = ['postTag', 'comment_body', 'comment_score', 'verified', 'is_gold', 'has_verified_email',\n",
    "       'link_karma', 'total_karma', 'created_utc', 'comment_karma']\n",
    "y = \"mod_deleted\"\n",
    "x_cont = x[2:]\n",
    "x_text = 'comment_body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   comment_score  verified  is_gold  has_verified_email  link_karma  \\\n",
      "0      28.597130       0.0 -0.32277            0.455035   -0.084809   \n",
      "1       7.747394       0.0 -0.32277            0.455035   -0.071857   \n",
      "2       4.718825       0.0 -0.32277            0.455035   -0.128258   \n",
      "3       2.339236       0.0 -0.32277            0.455035    0.022666   \n",
      "4       0.917663       0.0 -0.32277            0.455035    0.022504   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  actually      also  ...     still  \\\n",
      "0    -0.078767    -0.402122      -0.058956       0.0  0.000000  ...  0.000000   \n",
      "1     0.173269    -2.105970       0.260526       0.0  0.000000  ...  1.000000   \n",
      "2    -0.236544     0.515729      -0.246319       0.0  0.000000  ...  0.000000   \n",
      "3     1.475998     0.526639       1.973054       0.0  0.000000  ...  0.000000   \n",
      "4     0.102184    -1.309254       0.126044       0.0  0.338941  ...  0.667799   \n",
      "\n",
      "   take  things  think  time  want  way  white  would       ye  \n",
      "0   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.56772  \n",
      "1   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "2   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "3   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "4   0.0     0.0    0.0   0.0   0.0  0.0    0.0    0.0  0.00000  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "   comment_score  verified   is_gold  has_verified_email  link_karma  \\\n",
      "0      37.718721       0.0  3.432657            0.345678    1.783245   \n",
      "1       5.891780       0.0 -0.291320           -2.892862    0.770020   \n",
      "2       6.696299       0.0  3.432657            0.345678    1.783245   \n",
      "3       1.367940       0.0 -0.291320            0.345678   -0.107935   \n",
      "4       0.133782       0.0 -0.291320            0.345678   -0.119653   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  actually      also  ...  think  \\\n",
      "0     1.419133    -0.723235       0.784694       0.0  0.215382  ...    0.0   \n",
      "1     0.174602    -0.625795      -0.179065       0.0  0.000000  ...    0.0   \n",
      "2     1.419133    -0.723235       0.784694       0.0  0.000000  ...    0.0   \n",
      "3    -0.193661     0.466482      -0.185945       0.0  0.000000  ...    0.0   \n",
      "4    -0.341233    -0.047857      -0.357464       0.0  0.000000  ...    0.0   \n",
      "\n",
      "    though      time  use  water       way     well  work  would     years  \n",
      "0  0.27535  0.229500  0.0    0.0  0.250457  0.00000   0.0    0.0  0.000000  \n",
      "1  0.00000  0.000000  0.0    0.0  0.000000  0.00000   0.0    0.0  0.000000  \n",
      "2  0.00000  0.278241  0.0    0.0  0.000000  0.32142   0.0    0.0  0.320764  \n",
      "3  0.00000  0.000000  0.0    0.0  0.000000  0.00000   0.0    0.0  0.000000  \n",
      "4  0.00000  0.000000  0.0    0.0  0.000000  0.00000   0.0    0.0  0.000000  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "   comment_score  verified   is_gold  has_verified_email  link_karma  \\\n",
      "0      47.792197  0.007589 -0.301189            0.432884   -0.166731   \n",
      "1      13.514278  0.007589 -0.301189            0.432884   -0.174926   \n",
      "2       8.040238  0.007589 -0.301189           -2.310086   -0.110641   \n",
      "3       5.998334  0.007589  3.320173            0.432884    0.347713   \n",
      "4       1.545246  0.007589 -0.301189            0.432884   -0.103222   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  actually  also  ...   us  \\\n",
      "0     0.928256    -0.639019       1.153827       0.0   0.0  ...  0.0   \n",
      "1    -0.454548     1.050039      -0.463116       0.0   0.0  ...  0.0   \n",
      "2     0.226239     0.599811       0.302222       0.0   0.0  ...  0.0   \n",
      "3     1.957476    -1.633733       2.163684       0.0   0.0  ...  0.0   \n",
      "4    -0.145924     0.882896      -0.134305       0.0   0.0  ...  0.0   \n",
      "\n",
      "       vote    voters    voting  want  way  well  would  years  young  \n",
      "0  0.000000  0.385440  0.000000   0.0  0.0   0.0    0.0    0.0    0.0  \n",
      "1  0.000000  0.000000  0.000000   0.0  0.0   0.0    0.0    0.0    0.0  \n",
      "2  0.000000  0.000000  0.000000   0.0  0.0   0.0    0.0    0.0    0.0  \n",
      "3  0.000000  0.850512  0.288805   0.0  0.0   0.0    0.0    0.0    0.0  \n",
      "4  0.518752  0.000000  0.000000   0.0  0.0   0.0    0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "   comment_score  verified   is_gold  has_verified_email  link_karma  \\\n",
      "0      15.686952       0.0 -0.248405            0.330106   -0.215551   \n",
      "1       5.177045       0.0 -0.248405            0.330106   -0.220557   \n",
      "2       1.616893       0.0 -0.248405            0.330106   -0.075377   \n",
      "3       0.508343       0.0 -0.248405            0.330106   -0.221523   \n",
      "4      -0.003295       0.0 -0.248405           -3.029328   -0.220766   \n",
      "\n",
      "   total_karma  created_utc  comment_karma  also  american  ...  think  time  \\\n",
      "0    -0.449971     0.267562      -0.514162   0.0       0.0  ...    0.0   0.0   \n",
      "1    -0.247943     0.234639      -0.183669   0.0       0.0  ...    0.0   0.0   \n",
      "2     0.865078    -1.744504       1.545475   0.0       0.0  ...    0.0   0.0   \n",
      "3    -0.412380     0.252493      -0.445211   0.0       0.0  ...    0.0   0.0   \n",
      "4    -0.423187     0.338678      -0.465823   0.0       0.0  ...    0.0   0.0   \n",
      "\n",
      "         us      want  war  way  well     world  would  years  \n",
      "0  0.199777  0.260923  0.0  0.0   0.0  0.269937    0.0    0.0  \n",
      "1  0.000000  0.000000  0.0  0.0   0.0  0.000000    0.0    0.0  \n",
      "2  0.000000  0.000000  0.0  0.0   0.0  0.000000    0.0    0.0  \n",
      "3  0.000000  0.000000  0.0  0.0   0.0  0.000000    0.0    0.0  \n",
      "4  0.000000  0.545207  0.0  0.0   0.0  0.000000    0.0    0.0  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "#y is our target, what we are trying to predict. That is either deleted by mod or deleted by user (if deleted at all). Two splits.\n",
    "for df in dataframes:\n",
    "    tfidf = TfidfVectorizer(stop_words=stopwords.words('english'),max_df=.5, ngram_range=(1,2) ,max_features=50, strip_accents=\"unicode\")\n",
    "    # Use grouped split so that comments from a posts are not split between training & test set\n",
    "    X = df[x]\n",
    "    Y = df[y]\n",
    "    gs = GroupShuffleSplit(n_splits=2, test_size=.3, random_state=0)\n",
    "    train_ind, test_ind = next(gs.split(X, Y, groups=X.postTag))\n",
    "    X_train = X.iloc[train_ind].drop(\"postTag\", axis=1)\n",
    "    y_train = Y.iloc[train_ind]\n",
    "    X_test = X.iloc[test_ind].drop(\"postTag\", axis=1)\n",
    "    y_test = Y.iloc[test_ind]\n",
    "    \n",
    "    \"\"\"\n",
    "        Preprocess continuous columns and comment body\n",
    "        We may want to grid search with tfidf params instead of using above params\n",
    "    \"\"\"\n",
    "    cont_pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    text_pipe = Pipeline([\n",
    "        ('vect', tfidf)\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cont', cont_pipe, x_cont),\n",
    "        ('text', text_pipe, x_text)\n",
    "    ])\n",
    "    \n",
    "    preprocessor.fit(X_train)\n",
    "    columns = preprocessor.named_transformers_[\"text\"][0].get_feature_names_out()\n",
    "    columns = list(x_cont) + list(columns)\n",
    "    X_train_trans = pd.DataFrame(preprocessor.transform(X_train).toarray(), columns=columns)\n",
    "    print(X_train_trans.head()) # Transformed training data\n",
    "    \n",
    "    # Create model pipeline & param_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
      "0        0.007473      0.000315         0.015718        0.027965           0   \n",
      "1        0.007857      0.001354         0.001695        0.000935           0   \n",
      "2        0.006170      0.000682         0.002789        0.000810           0   \n",
      "3        0.147690      0.050007         0.001587        0.000198           0   \n",
      "4        0.219911      0.047829         0.002087        0.000796           0   \n",
      "5        0.006459      0.001992         0.001502        0.000323         0.5   \n",
      "6        0.003096      0.000493         0.001886        0.001058         0.5   \n",
      "7        0.004372      0.001685         0.002302        0.001177         0.5   \n",
      "8        0.134950      0.029353         0.002388        0.000574         0.5   \n",
      "9        0.200604      0.015021         0.001489        0.000314         0.5   \n",
      "10       0.006291      0.000510         0.001885        0.000853        0.25   \n",
      "11       0.003784      0.001171         0.001991        0.001307        0.25   \n",
      "12       0.003968      0.001086         0.001397        0.000378        0.25   \n",
      "13       0.144650      0.039210         0.002196        0.000921        0.25   \n",
      "14       0.203871      0.030901         0.001984        0.000702        0.25   \n",
      "15       0.005777      0.000749         0.001293        0.000246         0.1   \n",
      "16       0.003296      0.000522         0.001489        0.000313         0.1   \n",
      "17       0.003583      0.001240         0.002091        0.000973         0.1   \n",
      "18       0.148433      0.056658         0.001788        0.000864         0.1   \n",
      "19       0.200556      0.018578         0.002089        0.001463         0.1   \n",
      "20       0.008064      0.003268         0.001892        0.000202        0.01   \n",
      "21       0.004976      0.001778         0.001592        0.000199        0.01   \n",
      "22       0.005271      0.000920         0.001493        0.000009        0.01   \n",
      "23       0.161574      0.066986         0.001588        0.000372        0.01   \n",
      "24       0.208528      0.035781         0.001490        0.000005        0.01   \n",
      "25       0.005666      0.000865         0.001693        0.000685       0.001   \n",
      "26       0.002982      0.000450         0.001596        0.000583       0.001   \n",
      "27       0.003775      0.001432         0.001293        0.000246       0.001   \n",
      "28       0.169122      0.071893         0.001587        0.000486       0.001   \n",
      "29       0.221565      0.016964         0.001591        0.000197       0.001   \n",
      "30       0.005668      0.000387         0.001396        0.000383      0.0001   \n",
      "31       0.002679      0.000506         0.001496        0.000550      0.0001   \n",
      "32       0.003481      0.001298         0.001794        0.000680      0.0001   \n",
      "33       0.169737      0.072849         0.002297        0.001207      0.0001   \n",
      "34       0.179413      0.017311         0.001596        0.000372      0.0001   \n",
      "35       0.004679      0.000520         0.003486        0.002411         1.0   \n",
      "36       0.006869      0.008047         0.001290        0.000396         1.0   \n",
      "37       0.006175      0.004498         0.001589        0.000728         1.0   \n",
      "38       0.119307      0.016159         0.001389        0.000372         1.0   \n",
      "39       0.145293      0.016973         0.001491        0.000005         1.0   \n",
      "40       0.006069      0.000736         0.001290        0.000243        1.25   \n",
      "41       0.003385      0.000803         0.001987        0.000447        1.25   \n",
      "42       0.003390      0.000665         0.001488        0.000313        1.25   \n",
      "43       0.115323      0.018366         0.002092        0.000484        1.25   \n",
      "44       0.147279      0.031908         0.001593        0.000490        1.25   \n",
      "45       0.006762      0.002365         0.001795        0.000399         1.5   \n",
      "46       0.003285      0.000746         0.001589        0.000372         1.5   \n",
      "47       0.003677      0.000967         0.001896        0.000585         1.5   \n",
      "48       0.106362      0.010176         0.001191        0.000243         1.5   \n",
      "49       0.140716      0.022134         0.001891        0.000587         1.5   \n",
      "50       0.005672      0.000682         0.001389        0.000486           5   \n",
      "51       0.003091      0.001240         0.003085        0.002772           5   \n",
      "52       0.003278      0.000594         0.001496        0.000314           5   \n",
      "53       0.082483      0.015917         0.001290        0.000243           5   \n",
      "54       0.122872      0.028021         0.001590        0.000490           5   \n",
      "55       0.005167      0.000513         0.001397        0.000496          10   \n",
      "56       0.003386      0.000864         0.001493        0.000011          10   \n",
      "57       0.004084      0.001324         0.001791        0.001125          10   \n",
      "58       0.057996      0.010131         0.002092        0.000966          10   \n",
      "59       0.094001      0.014079         0.001292        0.000245          10   \n",
      "\n",
      "   param_solver                                   params  split0_test_score  \\\n",
      "0           svd            {'alpha': 0, 'solver': 'svd'}          -0.117065   \n",
      "1      cholesky       {'alpha': 0, 'solver': 'cholesky'}          -0.117065   \n",
      "2          lsqr           {'alpha': 0, 'solver': 'lsqr'}          -0.115004   \n",
      "3           sag            {'alpha': 0, 'solver': 'sag'}          -0.114307   \n",
      "4          saga           {'alpha': 0, 'solver': 'saga'}          -0.112598   \n",
      "5           svd          {'alpha': 0.5, 'solver': 'svd'}          -0.114524   \n",
      "6      cholesky     {'alpha': 0.5, 'solver': 'cholesky'}          -0.114524   \n",
      "7          lsqr         {'alpha': 0.5, 'solver': 'lsqr'}          -0.114121   \n",
      "8           sag          {'alpha': 0.5, 'solver': 'sag'}          -0.113211   \n",
      "9          saga         {'alpha': 0.5, 'solver': 'saga'}          -0.111796   \n",
      "10          svd         {'alpha': 0.25, 'solver': 'svd'}          -0.115125   \n",
      "11     cholesky    {'alpha': 0.25, 'solver': 'cholesky'}          -0.115125   \n",
      "12         lsqr        {'alpha': 0.25, 'solver': 'lsqr'}          -0.114546   \n",
      "13          sag         {'alpha': 0.25, 'solver': 'sag'}          -0.113937   \n",
      "14         saga        {'alpha': 0.25, 'solver': 'saga'}          -0.112335   \n",
      "15          svd          {'alpha': 0.1, 'solver': 'svd'}          -0.115533   \n",
      "16     cholesky     {'alpha': 0.1, 'solver': 'cholesky'}          -0.115533   \n",
      "17         lsqr         {'alpha': 0.1, 'solver': 'lsqr'}          -0.114816   \n",
      "18          sag          {'alpha': 0.1, 'solver': 'sag'}          -0.114108   \n",
      "19         saga         {'alpha': 0.1, 'solver': 'saga'}          -0.112485   \n",
      "20          svd         {'alpha': 0.01, 'solver': 'svd'}          -0.115799   \n",
      "21     cholesky    {'alpha': 0.01, 'solver': 'cholesky'}          -0.115799   \n",
      "22         lsqr        {'alpha': 0.01, 'solver': 'lsqr'}          -0.114985   \n",
      "23          sag         {'alpha': 0.01, 'solver': 'sag'}          -0.114239   \n",
      "24         saga        {'alpha': 0.01, 'solver': 'saga'}          -0.112495   \n",
      "25          svd        {'alpha': 0.001, 'solver': 'svd'}          -0.115827   \n",
      "26     cholesky   {'alpha': 0.001, 'solver': 'cholesky'}          -0.115827   \n",
      "27         lsqr       {'alpha': 0.001, 'solver': 'lsqr'}          -0.115002   \n",
      "28          sag        {'alpha': 0.001, 'solver': 'sag'}          -0.114270   \n",
      "29         saga       {'alpha': 0.001, 'solver': 'saga'}          -0.112644   \n",
      "30          svd       {'alpha': 0.0001, 'solver': 'svd'}          -0.115830   \n",
      "31     cholesky  {'alpha': 0.0001, 'solver': 'cholesky'}          -0.115830   \n",
      "32         lsqr      {'alpha': 0.0001, 'solver': 'lsqr'}          -0.115004   \n",
      "33          sag       {'alpha': 0.0001, 'solver': 'sag'}          -0.114263   \n",
      "34         saga      {'alpha': 0.0001, 'solver': 'saga'}          -0.112640   \n",
      "35          svd          {'alpha': 1.0, 'solver': 'svd'}          -0.113530   \n",
      "36     cholesky     {'alpha': 1.0, 'solver': 'cholesky'}          -0.113530   \n",
      "37         lsqr         {'alpha': 1.0, 'solver': 'lsqr'}          -0.113364   \n",
      "38          sag          {'alpha': 1.0, 'solver': 'sag'}          -0.112682   \n",
      "39         saga         {'alpha': 1.0, 'solver': 'saga'}          -0.111274   \n",
      "40          svd         {'alpha': 1.25, 'solver': 'svd'}          -0.113111   \n",
      "41     cholesky    {'alpha': 1.25, 'solver': 'cholesky'}          -0.113111   \n",
      "42         lsqr        {'alpha': 1.25, 'solver': 'lsqr'}          -0.113026   \n",
      "43          sag         {'alpha': 1.25, 'solver': 'sag'}          -0.112167   \n",
      "44         saga        {'alpha': 1.25, 'solver': 'saga'}          -0.111115   \n",
      "45          svd          {'alpha': 1.5, 'solver': 'svd'}          -0.112731   \n",
      "46     cholesky     {'alpha': 1.5, 'solver': 'cholesky'}          -0.112731   \n",
      "47         lsqr         {'alpha': 1.5, 'solver': 'lsqr'}          -0.112712   \n",
      "48          sag          {'alpha': 1.5, 'solver': 'sag'}          -0.111844   \n",
      "49         saga         {'alpha': 1.5, 'solver': 'saga'}          -0.110747   \n",
      "50          svd            {'alpha': 5, 'solver': 'svd'}          -0.109784   \n",
      "51     cholesky       {'alpha': 5, 'solver': 'cholesky'}          -0.109784   \n",
      "52         lsqr           {'alpha': 5, 'solver': 'lsqr'}          -0.110018   \n",
      "53          sag            {'alpha': 5, 'solver': 'sag'}          -0.109364   \n",
      "54         saga           {'alpha': 5, 'solver': 'saga'}          -0.109050   \n",
      "55          svd           {'alpha': 10, 'solver': 'svd'}          -0.108588   \n",
      "56     cholesky      {'alpha': 10, 'solver': 'cholesky'}          -0.108588   \n",
      "57         lsqr          {'alpha': 10, 'solver': 'lsqr'}          -0.108147   \n",
      "58          sag           {'alpha': 10, 'solver': 'sag'}          -0.108456   \n",
      "59         saga          {'alpha': 10, 'solver': 'saga'}          -0.108390   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0           -0.075303          -0.586350          -0.042350   \n",
      "1           -0.075303          -0.586350          -0.042350   \n",
      "2           -0.077908          -0.515288          -0.029379   \n",
      "3           -0.074618          -0.528309          -0.040802   \n",
      "4           -0.072475          -0.509537          -0.035709   \n",
      "5           -0.071526          -0.532327          -0.036032   \n",
      "6           -0.071526          -0.532327          -0.036032   \n",
      "7           -0.073552          -0.500877          -0.027043   \n",
      "8           -0.070603          -0.510788          -0.033990   \n",
      "9           -0.068558          -0.495421          -0.029935   \n",
      "10          -0.073510          -0.549175          -0.039224   \n",
      "11          -0.073510          -0.549175          -0.039224   \n",
      "12          -0.075675          -0.507854          -0.028164   \n",
      "13          -0.072563          -0.519396          -0.037015   \n",
      "14          -0.070442          -0.502570          -0.032481   \n",
      "15          -0.074744          -0.566040          -0.041538   \n",
      "16          -0.074744          -0.566040          -0.041538   \n",
      "17          -0.077001          -0.512257          -0.028881   \n",
      "18          -0.073760          -0.524896          -0.039259   \n",
      "19          -0.071637          -0.506730          -0.034400   \n",
      "20          -0.075500          -0.583378          -0.043118   \n",
      "21          -0.075500          -0.583378          -0.043118   \n",
      "22          -0.077817          -0.514981          -0.029329   \n",
      "23          -0.074593          -0.527655          -0.040693   \n",
      "24          -0.072289          -0.510846          -0.035682   \n",
      "25          -0.075576          -0.585712          -0.043285   \n",
      "26          -0.075576          -0.585712          -0.043285   \n",
      "27          -0.077899          -0.515257          -0.029374   \n",
      "28          -0.074627          -0.528210          -0.040772   \n",
      "29          -0.072408          -0.510395          -0.035522   \n",
      "30          -0.075584          -0.585954          -0.043302   \n",
      "31          -0.075584          -0.585954          -0.043302   \n",
      "32          -0.077907          -0.515285          -0.029379   \n",
      "33          -0.074631          -0.527873          -0.040903   \n",
      "34          -0.072375          -0.510448          -0.035797   \n",
      "35          -0.067814          -0.510650          -0.031268   \n",
      "36          -0.067814          -0.510650          -0.031268   \n",
      "37          -0.069602          -0.488150          -0.025053   \n",
      "38          -0.066940          -0.495785          -0.029583   \n",
      "39          -0.065080          -0.481614          -0.026340   \n",
      "40          -0.066078          -0.502153          -0.029422   \n",
      "41          -0.066078          -0.502153          -0.029422   \n",
      "42          -0.067761          -0.482336          -0.024168   \n",
      "43          -0.065246          -0.488547          -0.027895   \n",
      "44          -0.063524          -0.475235          -0.024750   \n",
      "45          -0.064417          -0.494535          -0.027826   \n",
      "46          -0.064417          -0.494535          -0.027826   \n",
      "47          -0.066004          -0.476851          -0.023349   \n",
      "48          -0.063596          -0.482522          -0.026307   \n",
      "49          -0.061829          -0.470433          -0.023736   \n",
      "50          -0.046949          -0.429724          -0.017199   \n",
      "51          -0.046949          -0.429724          -0.017199   \n",
      "52          -0.049332          -0.424041          -0.016481   \n",
      "53          -0.046413          -0.424366          -0.016598   \n",
      "54          -0.045328          -0.418993          -0.016340   \n",
      "55          -0.032640          -0.389896          -0.013583   \n",
      "56          -0.032640          -0.389896          -0.013583   \n",
      "57          -0.034051          -0.378890          -0.014530   \n",
      "58          -0.032350          -0.387410          -0.013513   \n",
      "59          -0.031732          -0.385318          -0.013491   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           -0.046452        -0.173504        0.208141               59  \n",
      "1           -0.046452        -0.173504        0.208141               59  \n",
      "2           -0.043805        -0.156277        0.181927               40  \n",
      "3           -0.044380        -0.160483        0.185798               48  \n",
      "4           -0.042029        -0.154470        0.179609               32  \n",
      "5           -0.042754        -0.159433        0.188487               43  \n",
      "6           -0.042754        -0.159433        0.188487               44  \n",
      "7           -0.040922        -0.151303        0.177344               25  \n",
      "8           -0.041188        -0.153956        0.180580               31  \n",
      "9           -0.039162        -0.148974        0.175558               22  \n",
      "10          -0.044393        -0.164286        0.194325               50  \n",
      "11          -0.044393        -0.164286        0.194325               49  \n",
      "12          -0.042322        -0.153712        0.179561               30  \n",
      "13          -0.042724        -0.157127        0.183177               41  \n",
      "14          -0.040625        -0.151691        0.177659               26  \n",
      "15          -0.045441        -0.168659        0.200448               52  \n",
      "16          -0.045441        -0.168659        0.200448               51  \n",
      "17          -0.043201        -0.155231        0.180962               36  \n",
      "18          -0.043685        -0.159142        0.184822               42  \n",
      "19          -0.041494        -0.153349        0.178821               29  \n",
      "20          -0.046096        -0.172778        0.206960               54  \n",
      "21          -0.046096        -0.172778        0.206960               53  \n",
      "22          -0.043744        -0.156171        0.181829               37  \n",
      "23          -0.044376        -0.160311        0.185559               45  \n",
      "24          -0.042091        -0.154680        0.180144               34  \n",
      "25          -0.046163        -0.173313        0.207848               56  \n",
      "26          -0.046163        -0.173313        0.207848               55  \n",
      "27          -0.043799        -0.156266        0.181917               38  \n",
      "28          -0.044359        -0.160448        0.185766               47  \n",
      "29          -0.042084        -0.154611        0.179969               33  \n",
      "30          -0.046170        -0.173368        0.207940               58  \n",
      "31          -0.046170        -0.173368        0.207940               57  \n",
      "32          -0.043805        -0.156276        0.181926               39  \n",
      "33          -0.044422        -0.160419        0.185608               46  \n",
      "34          -0.042152        -0.154682        0.179949               35  \n",
      "35          -0.039812        -0.152615        0.181305               27  \n",
      "36          -0.039812        -0.152615        0.181305               28  \n",
      "37          -0.038353        -0.146904        0.173309               18  \n",
      "38          -0.038427        -0.148684        0.175954               21  \n",
      "39          -0.036562        -0.144174        0.171271               15  \n",
      "40          -0.038483        -0.149849        0.178544               23  \n",
      "41          -0.038483        -0.149849        0.178544               24  \n",
      "42          -0.037172        -0.144893        0.171471               16  \n",
      "43          -0.037215        -0.146214        0.173659               17  \n",
      "44          -0.035440        -0.142013        0.169269               12  \n",
      "45          -0.037236        -0.147349        0.176082               19  \n",
      "46          -0.037236        -0.147349        0.176082               20  \n",
      "47          -0.036053        -0.142994        0.169739               13  \n",
      "48          -0.036036        -0.144061        0.171816               14  \n",
      "49          -0.034353        -0.140220        0.167828               11  \n",
      "50          -0.025455        -0.125822        0.155375                9  \n",
      "51          -0.025455        -0.125822        0.155375               10  \n",
      "52          -0.025111        -0.124997        0.153055                8  \n",
      "53          -0.024798        -0.124308        0.153512                7  \n",
      "54          -0.023836        -0.122710        0.151690                6  \n",
      "55          -0.017673        -0.112476        0.142911                4  \n",
      "56          -0.017673        -0.112476        0.142911                5  \n",
      "57          -0.016083        -0.110340        0.138570                1  \n",
      "58          -0.017330        -0.111812        0.142035                3  \n",
      "59          -0.016876        -0.111161        0.141356                2  \n",
      "-0.11178643720385603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.04023498412452442"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_test_trans = pd.DataFrame(preprocessor.transform(X_test).toarray(), columns=columns)\n",
    "\n",
    "param_grid = {'alpha': [0, 0.5, 0.25, 0.1, 0.01, 0.001, 0.0001, 1.0, 1.25, 1.5, 5, 10],  \n",
    "              \"solver\": ['svd', 'cholesky', 'lsqr', 'sag', 'saga']}\n",
    "    \n",
    "ridgeRegression = GridSearchCV(Ridge(), param_grid=param_grid, n_jobs=-1)\n",
    "ridgeRegression.fit(X_train_trans, y_train)\n",
    "ridgeRegression.best_params_\n",
    "results=pd.DataFrame(ridgeRegression.cv_results_)\n",
    "print(results)\n",
    "accuracy = cross_val_score(estimator=ridgeRegression, X=X_train_trans, y=y_train, cv=5)\n",
    "print(accuracy.mean())\n",
    "\n",
    "ridgeRegression.score(X_test_trans, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "054be14ecebcb39708ff65b21a327ce9d64d7b17e0e548e65f69c09e018835cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
